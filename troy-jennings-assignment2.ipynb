{"cells":[{"cell_type":"code","source":["! python -m pip install --upgrade pip\n! pip install TextBlob"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad9cd966-7ff7-4a14-898e-24a019efc4bf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Requirement already satisfied: pip in /databricks/python3/lib/python3.8/site-packages (22.0.4)\r\nRequirement already satisfied: TextBlob in /databricks/python3/lib/python3.8/site-packages (0.17.1)\r\nRequirement already satisfied: nltk>=3.1 in /databricks/python3/lib/python3.8/site-packages (from TextBlob) (3.6.1)\r\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (from nltk>=3.1->TextBlob) (4.59.0)\r\nRequirement already satisfied: regex in /databricks/python3/lib/python3.8/site-packages (from nltk>=3.1->TextBlob) (2021.4.4)\r\nRequirement already satisfied: click in /databricks/python3/lib/python3.8/site-packages (from nltk>=3.1->TextBlob) (7.1.2)\r\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk>=3.1->TextBlob) (1.0.1)\r\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Requirement already satisfied: pip in /databricks/python3/lib/python3.8/site-packages (22.0.4)\r\nRequirement already satisfied: TextBlob in /databricks/python3/lib/python3.8/site-packages (0.17.1)\r\nRequirement already satisfied: nltk>=3.1 in /databricks/python3/lib/python3.8/site-packages (from TextBlob) (3.6.1)\r\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.8/site-packages (from nltk>=3.1->TextBlob) (4.59.0)\r\nRequirement already satisfied: regex in /databricks/python3/lib/python3.8/site-packages (from nltk>=3.1->TextBlob) (2021.4.4)\r\nRequirement already satisfied: click in /databricks/python3/lib/python3.8/site-packages (from nltk>=3.1->TextBlob) (7.1.2)\r\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk>=3.1->TextBlob) (1.0.1)\r\n"]}}],"execution_count":0},{"cell_type":"code","source":["import nltk\nimport os\nimport re\n\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.stem import WordNetLemmatizer\nfrom pyspark import keyword_only\nfrom pyspark.context import SparkContext\nfrom pyspark.ml import Pipeline, Transformer\nfrom pyspark.ml.classification import LogisticRegression, NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import CountVectorizer, Word2Vec, StringIndexer\nfrom pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasOutputCols, Param, Params, TypeConverters\nfrom pyspark.ml.feature import SQLTransformer, StopWordsRemover, Tokenizer, CountVectorizer, HashingTF, IDF\nfrom pyspark.sql import functions as f\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.types import ArrayType, StructType, StructField, LongType, StringType, FloatType, TimestampType, IntegerType\nfrom textblob import TextBlob\n\nsc = SparkContext.getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66db425b-9101-4500-8394-96fccb2fece6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# download stems and sentiment lexicon\nnltk.download('wordnet')\nnltk.download('vader_lexicon')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"996f0640-1cc4-4d1f-8da8-6fa26ce0be64"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\nOut[3]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\nOut[3]: True"]}}],"execution_count":0},{"cell_type":"code","source":["DATA_PATH = '/FileStore/tables/training_tweets-1.csv'\nSTREAMING_PATH = '/tmp/stream_tweets/train'\n\n# define schema, read in data, and partition it to train/test\ntweet_schema = StructType([\n  StructField('author_id', LongType(), False),\n  StructField('tweet_created_at', TimestampType(), False),\n  StructField('tweet_id', LongType(), False),\n  StructField('tweet_text', StringType(), True),\n])\n\ntweet_df = spark.read.format('csv') \\\n                .schema(tweet_schema) \\\n                .option('header', True) \\\n                .option('mode', 'dropmalformed') \\\n                .load(DATA_PATH)\n\n# create a sample of 50K tweets to keep running times low\ndata = tweet_df.sample(withReplacement=False, fraction=0.1, seed=3)\ntrain, test = data.randomSplit(weights=[0.7, 0.3], seed=42)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d3a5462-9ee1-45a9-a570-546677329ab6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(f'Train shape: ({train.count()}, {len(train.columns)})')\nprint(f'Test shape: ({test.count()}, {len(test.columns)})')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74ae20cd-12ff-4a5e-b5bc-8c06c0004d29"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Train shape: (34604, 5)\nTest shape: (14504, 4)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Train shape: (34604, 5)\nTest shape: (14504, 4)\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Machine Learning"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d745451-8567-4267-9726-4e5733240331"}}},{"cell_type":"code","source":["class SentimentTransformer(Transformer, HasInputCol, HasOutputCol, HasOutputCols):\n\n    @keyword_only\n    def __init__(self, inputCol=None, outputCol=None, outputCols=None):\n        super().__init__()\n        kwargs = self._input_kwargs\n        self.setParams(**kwargs)\n\n    @keyword_only\n    def setParams(self, inputCol=None, outputCol=None, outputCols=None):\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)\n\n    def setInputCol(self, new_inputCol):\n        return self.setParams(inputCol=new_inputCol)\n\n    def setOutputCol(self, new_outputCol):\n        return self.setParams(outputCol=new_outputCol)\n      \n    def setOutputCols(self, new_outputCols):\n        return self.setParams(outputCols=new_outputCols)\n\n    def _transform(self, dataset):\n      if not self.isSet('inputCol'):\n          raise ValueError('No input column set for the SentimentTransformer transformer.')\n\n      input_col = self.getInputCol()\n      output_col = self.getOutputCol()\n      \n      def _score_sentiment(tokens):\n        '''Return a list of the neutral, negative, and positive sentiment values.'''\n        score = TextBlob(' '.join(tokens))\n        if score.sentiment.polarity < 0:\n          return 'negative'\n        elif score.sentiment.polarity == 0:\n          return 'neutral'\n        else:\n          return 'positive'\n      \n      # add sentiment scores to the dataset\n      scores = f.udf(lambda s: _score_sentiment(s), StringType())\n      return dataset.withColumn(output_col, scores(input_col))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b721ad9-b47d-4c37-8e1c-33a6ff8ddf62"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# ******************************************** #\n# **********    CUSTOM FUNCTIONS    ********** #\n# ******************************************** #\n\ndef is_retweet(tweet) -> str:\n  '''Indicates whether the tweet is a retweet.'''\n  retweet = False\n  if re.search(r'^RT ', tweet):\n    retweet = True\n  return retweet\n\ndef count_mentions(tweet) -> int:\n  '''Count the number of mentions (e.g. @TwitterDev) in the tweet text.'''\n  mention_matches = re.findall(r'@[\\w]+', tweet)\n  return len(mention_matches)\n\ndef count_links(tweet) -> int:\n  '''Count the number of links in the tweet'''\n  link_matches = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet)\n  return len(link_matches)\n\ndef get_uppercase_percentage(tweet) -> int:\n  '''Calculate the percentage of uppercase to lowercase letters.'''\n  uppercase_matches = re.findall(r'[A-Z]', tweet)\n  return len(uppercase_matches) / len(tweet)\n\ndef clean_tweet_text(tweet) -> str:\n  '''Cleanses a tweet text of user mentions, links, hashtags, special characters, and emojis.'''\n\n  replacement_patterns = [\n      r'(@[\\w]+)',\n      r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n      r'(#\\w+)',\n      r'^(RT )',\n      r'[\\$&+,:;=?@#|\\<>.^*()%!-/]',\n      r'\\n',\n      r'[0-9]+'\n  ]\n  clean_text = re.sub(r'|'.join(replacement_patterns), '', tweet, 99)\n\n  return clean_text.encode(encoding='ascii', errors='ignore').decode('ascii').strip()\n\ndef simple_tokenize_text(tweet) -> list:\n  '''Tokenizes the tweet text (with support for multiple spaces in-string)'''\n  return [ t.lower() for t in tweet.split() ]\n\ndef lemmatize_tokens(tokens) -> list:\n  '''Returns the base form of each word in a list of tokens.'''\n  wn_lemmatizer = WordNetLemmatizer()\n  return [ wn_lemmatizer.lemmatize(word, pos='v') for word in tokens ]\n\n\n# ******************************************** #\n# ********** USER-DEFINED FUNCTIONS ********** #\n# ******************************************** #\n\nclean_text_udf = spark.udf.register('clean_text_udf', lambda row: clean_tweet_text(row), StringType())\ntokenizer_udf = spark.udf.register('tokenizer_udf', lambda row: simple_tokenize_text(row), ArrayType(StringType()))\nlemmatizer_udf = spark.udf.register('lemmatizer_udf', lambda row: lemmatize_tokens(row), ArrayType(StringType()))\n# negative_indicator_udf = spark.udf.register('negative_indicator_udf', lambda row: classify_negative_sentiment(row), FloatType())\n\n# ******************************************** #\n# **********      TRANSFORMERS      ********** #\n# ******************************************** #\n\nfilter_na = SQLTransformer(statement=\"SELECT * FROM __THIS__ WHERE tweet_id IS NOT NULL AND tweet_text IS NOT NULL\")\nclean_text_transformer = SQLTransformer(statement=\"SELECT *, clean_text_udf(tweet_text) AS cleansed FROM __THIS__\")\nfilter_nulls_transformer = SQLTransformer(statement=\"SELECT * FROM __THIS__ WHERE cleansed IS NOT NULL AND cleansed != ''\")\ntokenizer = SQLTransformer(statement=\"SELECT *, tokenizer_udf(cleansed) as tokens FROM __THIS__\")\nstop_remover = StopWordsRemover(stopWords=StopWordsRemover().getStopWords()).setInputCol('tokens').setOutputCol('stops_removed')\nsentiment_scorer = SentimentTransformer().setInputCol('lemmatized').setOutputCol('sentiment')\nprediction_indexer = StringIndexer(inputCol='sentiment', outputCol='label')\nlemmatizer = SQLTransformer(statement=\"SELECT *, lemmatizer_udf(stops_removed) AS lemmatized FROM __THIS__\")\nhashing_tf = HashingTF().setInputCol('lemmatized').setOutputCol('raw_features').setNumFeatures(20)\nidf = IDF().setInputCol('raw_features').setOutputCol('features')\nnb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"128f09bf-67ea-4caa-8377-95117a4e6fab"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create a training pipeline\npreproc_pipeline = Pipeline(stages=[\n  filter_na,\n  clean_text_transformer,\n  filter_nulls_transformer,\n  tokenizer,\n  stop_remover,\n  lemmatizer,\n  hashing_tf,\n  idf,\n  sentiment_scorer,\n  prediction_indexer,\n  nb\n])\n\n# build the model\nnb_model = preproc_pipeline.fit(train)\ntrain_results = nb_model.transform(train)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35dc78a7-968a-48fc-b547-e58ce5907e9c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# evaluate training data\nclassificationEval = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName=\"accuracy\")\ntrain_eval = classificationEval.evaluate(train_results)\nprint(f'Training Classification Score (Accuracy) is {train_eval}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7f7f956-8184-4ab2-a4a6-02cb2a4ae7b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Training Classification Score (Accuracy) is 0.5242597009715516\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Training Classification Score (Accuracy) is 0.5242597009715516\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Streaming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c35cfe9d-2dd7-4aff-aa22-2191b80d562c"}}},{"cell_type":"code","source":["# do date preprocessing to partition by hour\nhour_udf = f.udf(lambda x: x.hour, IntegerType())\ntrain = train.withColumn('hour', hour_udf('tweet_created_at'))\n\n# write to disk\ntrain.write.option('header', True).partitionBy('hour').csv(STREAMING_PATH)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd2f451f-3f51-454b-bc8a-078f95b9fb27"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# stream in additional tweets\nstreaming_schema = StructType([\n  StructField('author_id', LongType(), False),\n  StructField('tweet_created_at', TimestampType(), False),\n  StructField('tweet_id', LongType(), False),\n  StructField('tweet_text', StringType(), True),\n  StructField('hour', IntegerType(), True),\n])\n\n# source\ntweet_stream = spark.readStream.format('csv') \\\n                    .option('header', True) \\\n                    .schema(streaming_schema) \\\n                    .option('mode', 'dropMalformed') \\\n                    .option('maxFilesPerTrigger', 1) \\\n                    .load(STREAMING_PATH)\n\n# query\npredict_tweet_sentiment = nb_model.transform(tweet_stream).select('tweet_id', 'probability', 'prediction')\n\n# sink\nsink_stream = predict_tweet_sentiment.writeStream.outputMode('append') \\\n                    .format('memory') \\\n                    .queryName('predict_tweet_sentiment') \\\n                    .trigger(processingTime='5 seconds') \\\n                    .start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eda8f103-0c73-49a6-a023-4da6f7e03c1c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql('SELECT SUM(CASE WHEN prediction = 1.0 THEN 1 ELSE 0 END) AS positive, SUM(CASE WHEN prediction = 0.0 THEN 1 ELSE 0 END) AS neutral, SUM(CASE WHEN prediction = 2.0 THEN 1 ELSE 0 END) AS negative  from predict_tweet_sentiment').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07bdee06-572d-441b-8f09-b06117818041"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+--------+-------+--------+\n|positive|neutral|negative|\n+--------+-------+--------+\n|    6586|  26809|     983|\n+--------+-------+--------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+--------+-------+--------+\n|positive|neutral|negative|\n+--------+-------+--------+\n|    6586|  26809|     983|\n+--------+-------+--------+\n\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"troy-jennings-assignment2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2619534905752387}},"nbformat":4,"nbformat_minor":0}
